class: center, middle

# Dimensionality Reduction Techniques

![Default-aligned image](http://scikit-learn.org/stable/_images/plot_digits_pipe_001.png)

---
# Outline

### High Dimensional Data
### What is Dimensionality Reduction?
### Why is DR useful?
### Examples of DR
### Analysis and Performance

---
# High Dimensional Data

- Hard to visualize.
- Difficult to sample.
- Computationally complex.
- Difficult to estimate.

![Multivariate Gaussian](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Gaussian_2d.svg/2000px-Gaussian_2d.svg.png)


---

# What is Dimensionality Reduction?

- Dimensionality reduction refers to a class of techniques which map higher dimensional data to lower dimensional approximations.
- The resulting representation is normally referred to as a _"low dimensional embedding"_

## Feature Selection

You are given an anonymized dataset. The features are all unnamed. Some are scalar values, some are categorical. Your job is to predict the value of some [unamed] scalar. How can you tell which features are useful and which are not?


---

class: center, middle

# `\(\LaTeX{}\)` in remark

---

# Display and Inline

1. This is an inline integral: `\(\int_a^bf(x)dx\)`
2. More `\(x={a \over b}\)` formulae.

Display formula:

$$e^{i\pi} + 1 = 0$$

---
class: center, middle
# _FIN_
