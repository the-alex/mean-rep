class: center, middle

# Dimensionality Reduction Techniques

![Default-aligned image](http://scikit-learn.org/stable/_images/plot_digits_pipe_001.png)

---

# Agenda

--

1. Introduction
    - Feature Selection
    - Curse of Dimensionality
2. Deep-dive
    - Three methods to deal with this.
3. ...

---

# Introduction

## Feature Selection

You are given an anonymized dataset. The features are all unnamed. Some are scalar values, some are categorical. Your job is to predict the value of some [unamed] scalar. How can you tell which features are useful and which are not?


---

class: center, middle

# `\(\LaTeX{}\)` in remark

---

# Display and Inline

1. This is an inline integral: `\(\int_a^bf(x)dx\)`
2. More `\(x={a \over b}\)` formulae.

Display formula:

$$e^{i\pi} + 1 = 0$$

---
class: center, middle
# _FIN_
